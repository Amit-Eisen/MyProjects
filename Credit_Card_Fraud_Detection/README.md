# ğŸš€ Credit Card Fraud Detection using XGBoost

## ğŸ” Overview
This project is an **end-to-end pipeline** for detecting fraudulent credit card transactions using **machine learning**.  
We use **XGBoost**, a powerful gradient boosting algorithm, to achieve high accuracy while maintaining a **low false-positive rate**.

âœ… **Dataset Source:** [Credit Card Fraud Detection â€“ Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)  
âœ… **Dataset Size:** ~284,807 transactions  
âœ… **Class Imbalance:** Only **0.17%** of transactions are fraudulent.  

Since the dataset is highly imbalanced, we use **optimized hyperparameters** instead of data augmentation techniques like SMOTE, which previously led to overfitting.

---

## ğŸ“‚ Project Structure
```
Credit_Card_Fraud_Detection/
â”‚â”€â”€ data/ # Raw and processed data 
â”‚ â”œâ”€â”€ raw/ # Original dataset (not uploaded to GitHub) 
â”‚ â”œâ”€â”€ processed/ # Preprocessed and split data 
â”‚â”€â”€ models/ # Trained models 
â”‚ â”œâ”€â”€ xgboost.pkl # Final trained XGBoost model 
â”‚â”€â”€ src/ # Source code 
â”‚ â”œâ”€â”€ explore_data.py # Data exploration and visualization 
â”‚ â”œâ”€â”€ preprocess.py # Data preprocessing (scaling, splitting) 
â”‚ â”œâ”€â”€ train_model.py # Training XGBoost model (with and without tuning) 
â”‚ â”œâ”€â”€ evaluate_model.py # Model evaluation and performance metrics 
â”‚â”€â”€ main.py # Runs the full pipeline 
â”‚â”€â”€ requirements.txt # Required libraries 
â”‚â”€â”€ README.md # Project documentation 
â”‚â”€â”€ .gitignore # Files to ignore in version control
```


---

## ğŸ“Š **Dataset Overview**
| Feature | Description |
|---------|------------|
| **Time** | Seconds elapsed since first transaction |
| **V1-V28** | PCA-transformed features (original features are not available) |
| **Amount** | Transaction amount |
| **Class** | 0 = Legitimate, 1 = Fraudulent |

ğŸ’¡ **The data is heavily imbalanced:**  
- **Non-fraudulent transactions (Class 0):** 284,315 (~99.83%)  
- **Fraudulent transactions (Class 1):** 492 (~0.17%)  

---
## **ğŸ“Š Model Performance**
### **Manual Hyperparameter Optimization (Best Results)**
| Model Type | Precision (Fraud) | Recall (Fraud) | F1-Score | ROC-AUC |
|------------|------------------|---------------|---------|--------|
| **XGBoost (Manual Params)** | **0.88** | **0.85** | **0.86** | **0.923** |
| XGBoost (Fine-Tuned) | 0.87 | 0.84 | 0.85 | 0.918 |

âœ… **We found that manually selecting hyperparameters gives slightly better results than Fine-Tuning.**  
âœ… **However, the code allows both methods so you can experiment.**  

---
## âš™ï¸ **How to Run the Project**
#### **1ï¸âƒ£ Install dependencies**
```bash
pip install -r requirements.txt
```
#### **2ï¸âƒ£ Run the full pipline (with manual parameters):**
```bash
python main.py
```
#### **3ï¸âƒ£ Run Fine-Tuning (optional, takes longer):**
```bash
python main.py --fine_tune
```
#### **4ï¸âƒ£ Evaluate the trained model:**
```bash
python src/evaluate_model.py
```

## Fine-Tuning vs. Manual Parameters:
  - **Manual Optimization:**
       - Pros:
         - Faster training
         - Better interpretability
       - Cons:
         - Requires domain knowledge


  - **Fine-Tuning (RandomizedSearchCV):**
       - Pros:
         - Automated process
         - Finds potentially better hyperparameters
       - Cons:
          - Longer runtime
          - Slight risk of overfitting

## ğŸ“ˆ Technologies Used
 - Python
 - XGBoost (for fraud detection)
 - Pandas & NumPy (for data processing)
 - Scikit-Learn (for model evaluation)
 - Matplotlib & Seaborn (for data visualization)
 - Joblib (for model saving/loading)

## ğŸš€ Future Improvements
 - Compare XGBoost with LightGBM to test performance.
 - Try AutoML (like Optuna) for hyperparameter optimization.
 - Develop a real-time fraud detection API using FastAPI.
 - Implement anomaly detection techniques (Isolation Forest, One-Class SVM).
